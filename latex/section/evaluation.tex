\section{Evaluation}
\subsection{Criteria}
For our evaluation of the different testing strategies, we need some criteria. With the help of these criteria we can rate and compare the strategies. \\
Below is the list of our criteria with explanations:
\begin{itemize}
\item control during runtime... amount of impact of the test-suite during execution
\item ensure quality before deploy... how much quality can the test-suite ensure before the system gets deployed
\item performance overhead... amount of additional effort for running the test-suite simultanously to the system, which includes time and memory
\item testing-cost... how complex is building the test for the developer
\item adaptability... how easy can the test-suite be adapted to an other system
\end{itemize}

\subsection{Comparison}

\subsubsection{Testmanager}
\begin{figure}[h]
	\includegraphics[height=7cm]
	{images/testmanager.png}
	\caption{testmanager - Kiviat graph}
	\label{testmngrKiviat}
\end{figure}
At first we want to evaluate the testmanager approach.
A testmanager has the ability to handle adaptation of the system and therefore its runtime control is high. It has not the best possible score, because there are situation, where the manager can not handle a test violoation (e.g. injured response time constraint). On the other hand it can not ensure good behavior prior to release, because there are no tests before deployment. All tests were performed at runtime, which causes an overhead. By using the ``safe adaption with validation`` method there is a higher time overhead and by using the ``replication with validation`` methode there is a bigger memory overhead. Due to its independance from the monitored system, the testmanager is relatively flexible and can be used as an independent component. A system developer would need to define the constraints for each situation and the testmanager would automatically perform the required tests at runtime. Defining these constraints can be very time-consuming, if there are multiple different environments and actors that need to be included in the testing process.


\subsubsection{Corridor enforcing infrastructure}
\begin{figure}[h]
	\includegraphics[height=7cm]
	{images/corridor.png}
	\caption{corridor enforcing infrastructure - Kiviat graph}
	\label{corridorKiviat}
\end{figure}

\noindent+ Like the testmanager, the corridor enforcing infrastracture (CEI) is also able to adapt to a new system-state and can execute well fitting tests. As well as the testmanager it has not the highest possible score, because the CEI might not always be able to recover a good system-state. The CEI highly depends on the system, because the CEI must know possible actions of the system to bring it back to a save state. This reduces the adaptability and makes the testing-costs very high. As a result of the big infrastructure that need to run, there is a performance overhead. One advantage of the CEI over the testmanager is, that it can ensure quality before deploy to some degree. The reason is that the CEI itself can be tested before deploy. If one can show that the CEI is working it will hold the system within the corridor of correct behavior. But testing the CEI is difficult, because one has to simulate system behavior and check the CEIs reactions.

\subsubsection{modelbased testing}
\begin{figure}[h]
	\includegraphics[height=7cm]
	{images/modelBased.png}
	\caption{model based testing - Kiviat graph}
	\label{modelbasedKiviat}
\end{figure}

